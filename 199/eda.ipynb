{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('anaconda3': virtualenv)",
   "metadata": {
    "interpreter": {
     "hash": "53a728046be83de3d19e5c08d70c5e324e411643d64f859255094c936e26e9ab"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import tweepy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the data -- it should be stored in a dirrectory called data\n",
    "politifact_fake = pd.read_csv('data/politifact_fake.csv')\n",
    "politifact_real = pd.read_csv('data/politifact_real.csv')\n",
    "\n",
    "gossipcop_fake = pd.read_csv('data/gossipcop_fake.csv')\n",
    "gossipcop_real = pd.read_csv('data/gossipcop_real.csv')"
   ]
  },
  {
   "source": [
    "## looking at shape and columns "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "politifact fake:(432, 4)\npolitifact real:(624, 4)\ngossip cop fake(4898, 4)\ngossip cop real(15747, 4)\n"
     ]
    }
   ],
   "source": [
    "# looking at shape and columns \n",
    "print(\"politifact fake:\" + str(politifact_fake.shape)) # 432 pieces of news  (fake)\n",
    "print(\"politifact real:\" + str(politifact_real.shape)) # 624 pieces of news (real)\n",
    "\n",
    "print(\"gossip cop fake\" + str(gossip_cop_fake.shape)) # 5323 pieces of news (fake)\n",
    "print(\"gossip cop real\" + str(gossip_cop_real.shape)) # 16817 peices of news (real)"
   ]
  },
  {
   "source": [
    "## drop na values \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop na values \n",
    "politifact_fake.dropna(inplace=True)\n",
    "politifact_real.dropna(inplace=True)\n",
    "\n",
    "gossip_cop_fake.dropna(inplace=True) \n",
    "gossip_cop_real.dropna(inplace=True)"
   ]
  },
  {
   "source": [
    "## looking at shape and columns after dropping NA values \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "politifact fake:(389, 4)\npolitifact real:(373, 4)\ngossip cop fake:(4898, 4)\ngossip cop real:(15747, 4)\n"
     ]
    }
   ],
   "source": [
    "# looking at shape and columns after dropping NA values \n",
    "print(\"politifact fake:\" + str(politifact_fake.shape)) # 389 pieces of news  (fake)\n",
    "print(\"politifact real:\" + str(politifact_real.shape)) # 373 pieces of news (real)\n",
    "\n",
    "print(\"gossip cop fake:\" + str(gossip_cop_fake.shape)) # 4898 pieces of news (fake)\n",
    "print(\"gossip cop real:\" + str(gossip_cop_real.shape)) # 15747 pieces of news (real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['id', 'news_url', 'title', 'tweet_ids'], dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "# column names\n",
    "politifact_fake.columns"
   ]
  },
  {
   "source": [
    "## columns\n",
    "- id is the id associated with a peice of news\n",
    "- news_url is a link to the original news \n",
    "- title is the title associated with the news \n",
    "- tweet_ids is a tab seperated string of all the tweet ids associated with that piece of news"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## converting the string of tweet ids to a list"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "politifact_fake['tweet_ids'] = politifact_fake.tweet_ids.str.split('\\t')\n",
    "politifact_real['tweet_ids'] = politifact_real.tweet_ids.str.split('\\t')\n",
    "gossipcop_fake['tweet_ids'] = gossipcop_fake.tweet_ids.str.split('\\t')\n",
    "gossipcop_real['tweet_ids'] = gossipcop_real.tweet_ids.str.split('\\t')\n"
   ]
  },
  {
   "source": [
    "## twitter API stuff"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = \"SfH6LhmYyWltgdlTFuEm3ODvr\"\n",
    "consumer_secret = \"WfzZyfovKaKuhjPV1EGGlTF9ha5zxuTGjIQIgdQ96GicdLyfxY\"\n",
    "access_token = \"1311486910322286592-yXQQ8l6IPmexbdK5gBgGGyo3QL7ZVR\"\n",
    "access_token_secret = \"U84aUFUgge2MLonznCKd3bodRedwT8bBi4NO6fAe2sHhP\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "source": [
    "## function for getting less than 100 tweets (API limit is 100 tweets at a time)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_small(row_data, number_of_tweets = False):\n",
    "    # attributes destruuctured \n",
    "    news_id = row_data.id \n",
    "    news_url = row_data.news_url\n",
    "    title = row_data.title\n",
    "    id_list = row_data.tweet_ids\n",
    "    # lists to store values in \n",
    "    tweet_ids_list = []\n",
    "    username_list = []\n",
    "    text_list = []\n",
    "    news_id_list = []\n",
    "    news_url_list = []\n",
    "    title_list = []\n",
    "    reply_list = []  \n",
    "    if number_of_tweets:\n",
    "        if (len(row_data.tweet_ids) > number_of_tweets + 1):\n",
    "            id_list = row_data.tweet_ids[:number_of_tweets + 1]\n",
    "    result = api.statuses_lookup(id_ = id_list)\n",
    "    for status in result:\n",
    "        # this is the ID for each individual tweet\n",
    "        tweet_id = status.id\n",
    "        tweet_ids_list.append(tweet_id)\n",
    "        # username for each individual tweet\n",
    "        username = status.user.screen_name \n",
    "        username_list.append(username)\n",
    "        # text value for each individual tweet\n",
    "        text = status.text\n",
    "        text_list.append(text)\n",
    "        # \n",
    "        reply = status.in_reply_to_status_id_str\n",
    "        if reply is None:\n",
    "            reply_list.append(False)\n",
    "        else:\n",
    "            reply_list.append(True)\n",
    "        # these are repeated values: news_id, news_url and title will be the same for all tweets associated with a particular news story (row) \n",
    "        news_id_list.append(news_id)\n",
    "        news_url_list.append(news_url)\n",
    "        title_list.append(title)\n",
    "    data = {\"news_id\": news_id_list, \"news_url\": news_url_list, \"title\": title_list, \"tweet_id\": tweet_ids_list, \"username\": username_list, \"text\": text_list, \"reply\": reply_list}\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "## function for getting more than 100 tweets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_large(row_data, number_of_tweets = False):\n",
    "\n",
    "    # attributes destruuctured \n",
    "    news_id = row_data.id \n",
    "    news_url = row_data.news_url\n",
    "    title = row_data.title\n",
    "    id_list = row_data.tweet_ids\n",
    "    # lists to store values in \n",
    "    tweet_ids_list = []\n",
    "    username_list = []\n",
    "    text_list = []\n",
    "    news_id_list = []\n",
    "    news_url_list = []\n",
    "    title_list = []\n",
    "    reply_list = []\n",
    "\n",
    "    if number_of_tweets:\n",
    "        if number_of_tweets < 100: # (len(row_data.tweet_ids) > number_of_tweets + 1) and\n",
    "            i = 1\n",
    "            tweet_subset = row_data.tweet_ids[:number_of_tweets + 1]\n",
    "    else: \n",
    "        i = len(row_data.tweet_ids) // 100\n",
    "        tweet_subset = row_data.tweet_ids[:99]\n",
    "\n",
    "    for j in range(i):\n",
    "        if number_of_tweets:\n",
    "            tweet_subset = row_data.tweet_ids[:number_of_tweets + 1]\n",
    "        else:\n",
    "            tweet_subset = row_data.tweet_ids[j*100:j*100+99]\n",
    "        result = api.statuses_lookup(tweet_subset)\n",
    "\n",
    "        for status in result:\n",
    "            # this is the ID for each individual tweet\n",
    "            tweet_id = status.id\n",
    "            tweet_ids_list.append(tweet_id)\n",
    "            # username for each individual tweet\n",
    "            username = status.user.screen_name \n",
    "            username_list.append(username)\n",
    "            # text value for each individual tweet\n",
    "            text = status.text\n",
    "            text_list.append(text)\n",
    "            # \n",
    "            reply = status.in_reply_to_status_id_str\n",
    "            if reply is None:\n",
    "                reply_list.append(False)\n",
    "            else:\n",
    "                reply_list.append(True)\n",
    "            # these are repeated values: news_id, news_url and title will be the same for all tweets associated with a particular news story (row) \n",
    "            news_id_list.append(news_id)\n",
    "            news_url_list.append(news_url)\n",
    "            title_list.append(title)\n",
    "    data = {\"news_id\": news_id_list, \"news_url\": news_url_list, \"title\": title_list, \"tweet_id\": tweet_ids_list, \"username\": username_list, \"text\": text_list, \"reply\": reply_list}\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "source": [
    "## function to aggregate all data in the data frame from "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_data(df, name = \"temp.csv\", number_of_tweets = False):\n",
    "    result = pd.DataFrame()\n",
    "    for row in df.iterrows():\n",
    "        row = row[1] # because iterrows() gives us a tuple (index, row) \n",
    "        if len(row.tweet_ids) < 100:\n",
    "            df2 = get_tweets_small(row_data = row, number_of_tweets = number_of_tweets)\n",
    "        else:\n",
    "            df2 = get_tweets_large(row_data = row, number_of_tweets = number_of_tweets)\n",
    "            \n",
    "            \n",
    "        # df2 = get_all_tweets_single_news_story(row_data = row, number_of_tweets = number_of_tweets)\n",
    "        # print(row)\n",
    "        result = result.append(df2, ignore_index= True)\n",
    "    # result.to_csv(name)\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# politifact_fake_test = politifact_fake.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "31\n61\n163\n"
     ]
    }
   ],
   "source": [
    "for row in politifact_fake_test.iterrows():\n",
    "    print(len(row[1].tweet_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_test_data = aggregate_data(politifact_fake_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_test_data.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(105, 7)"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "aggregated_test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test cases \n",
    "large_test = politifact_fake.iloc[2]\n",
    "test_df_large_subset = get_tweets_large(large_test, number_of_tweets = 15)\n",
    "test_df_large = get_tweets_large(large_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(155, 7)"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "test_df_large.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(12, 7)"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "test_df_large_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(155, 7)"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "test_df_large.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_test = politifact_fake.iloc[110]\n",
    "test_df_small = get_tweets_small(small_test)\n",
    "test_df_small_subset = get_tweets_small(small_test, number_of_tweets = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(48, 7)"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "test_df_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(20, 7)"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "test_df_small_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(48, 7)"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}